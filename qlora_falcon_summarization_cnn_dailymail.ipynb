{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "87ede209-9b96-4e1f-a92f-b7e4bd2a6592",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in c:\\users\\admin\\miniconda3\\envs\\llm-env\\lib\\site-packages (4.52.4)\n",
      "Requirement already satisfied: datasets in c:\\users\\admin\\miniconda3\\envs\\llm-env\\lib\\site-packages (3.6.0)\n",
      "Requirement already satisfied: peft in c:\\users\\admin\\miniconda3\\envs\\llm-env\\lib\\site-packages (0.15.2)\n",
      "Requirement already satisfied: accelerate in c:\\users\\admin\\miniconda3\\envs\\llm-env\\lib\\site-packages (1.8.1)\n",
      "Requirement already satisfied: bitsandbytes in c:\\users\\admin\\miniconda3\\envs\\llm-env\\lib\\site-packages (0.46.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\admin\\miniconda3\\envs\\llm-env\\lib\\site-packages (from transformers) (3.17.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in c:\\users\\admin\\miniconda3\\envs\\llm-env\\lib\\site-packages (from transformers) (0.31.1)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\admin\\miniconda3\\envs\\llm-env\\lib\\site-packages (from transformers) (2.0.1)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\admin\\miniconda3\\envs\\llm-env\\lib\\site-packages (from transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\admin\\miniconda3\\envs\\llm-env\\lib\\site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\admin\\miniconda3\\envs\\llm-env\\lib\\site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in c:\\users\\admin\\miniconda3\\envs\\llm-env\\lib\\site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in c:\\users\\admin\\miniconda3\\envs\\llm-env\\lib\\site-packages (from transformers) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in c:\\users\\admin\\miniconda3\\envs\\llm-env\\lib\\site-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\admin\\miniconda3\\envs\\llm-env\\lib\\site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\admin\\miniconda3\\envs\\llm-env\\lib\\site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2025.3.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\admin\\miniconda3\\envs\\llm-env\\lib\\site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.12.2)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in c:\\users\\admin\\miniconda3\\envs\\llm-env\\lib\\site-packages (from datasets) (20.0.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in c:\\users\\admin\\miniconda3\\envs\\llm-env\\lib\\site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in c:\\users\\admin\\miniconda3\\envs\\llm-env\\lib\\site-packages (from datasets) (2.2.3)\n",
      "Requirement already satisfied: xxhash in c:\\users\\admin\\miniconda3\\envs\\llm-env\\lib\\site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in c:\\users\\admin\\miniconda3\\envs\\llm-env\\lib\\site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in c:\\users\\admin\\miniconda3\\envs\\llm-env\\lib\\site-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.12.6)\n",
      "Requirement already satisfied: psutil in c:\\users\\admin\\miniconda3\\envs\\llm-env\\lib\\site-packages (from peft) (5.9.0)\n",
      "Requirement already satisfied: torch>=1.13.0 in c:\\users\\admin\\miniconda3\\envs\\llm-env\\lib\\site-packages (from peft) (2.7.1)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\admin\\miniconda3\\envs\\llm-env\\lib\\site-packages (from torch>=1.13.0->peft) (1.14.0)\n",
      "Requirement already satisfied: networkx in c:\\users\\admin\\miniconda3\\envs\\llm-env\\lib\\site-packages (from torch>=1.13.0->peft) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\admin\\miniconda3\\envs\\llm-env\\lib\\site-packages (from torch>=1.13.0->peft) (3.1.6)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in c:\\users\\admin\\miniconda3\\envs\\llm-env\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\admin\\miniconda3\\envs\\llm-env\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.3.2)\n",
      "Requirement already satisfied: async-timeout<6.0,>=4.0 in c:\\users\\admin\\miniconda3\\envs\\llm-env\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (5.0.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\admin\\miniconda3\\envs\\llm-env\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (24.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\admin\\miniconda3\\envs\\llm-env\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.6.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\admin\\miniconda3\\envs\\llm-env\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.4.4)\n",
      "Requirement already satisfied: propcache>=0.2.0 in c:\\users\\admin\\miniconda3\\envs\\llm-env\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.3.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in c:\\users\\admin\\miniconda3\\envs\\llm-env\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.20.0)\n",
      "Requirement already satisfied: idna>=2.0 in c:\\users\\admin\\miniconda3\\envs\\llm-env\\lib\\site-packages (from yarl<2.0,>=1.17.0->aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.7)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\admin\\miniconda3\\envs\\llm-env\\lib\\site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\admin\\miniconda3\\envs\\llm-env\\lib\\site-packages (from requests->transformers) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\admin\\miniconda3\\envs\\llm-env\\lib\\site-packages (from requests->transformers) (2025.4.26)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\admin\\miniconda3\\envs\\llm-env\\lib\\site-packages (from sympy>=1.13.3->torch>=1.13.0->peft) (1.3.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\admin\\miniconda3\\envs\\llm-env\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\admin\\miniconda3\\envs\\llm-env\\lib\\site-packages (from jinja2->torch>=1.13.0->peft) (3.0.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\admin\\miniconda3\\envs\\llm-env\\lib\\site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\admin\\miniconda3\\envs\\llm-env\\lib\\site-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\admin\\miniconda3\\envs\\llm-env\\lib\\site-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\admin\\miniconda3\\envs\\llm-env\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install transformers datasets peft accelerate bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f683f5ce-92c5-4800-be8a-73f76b82dffd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'article': 'LONDON, England (Reuters) -- Harry Potter star Daniel Radcliffe gains access to a reported Â£20 million ($41.1 million) fortune as he turns 18 on Monday, but he insists the money won\\'t cast a spell on him. Daniel Radcliffe as Harry Potter in \"Harry Potter and the Order of the Phoenix\" To the disappointment of gossip columnists around the world, the young actor says he has no plans to fritter his cash away on fast cars, drink and celebrity parties. \"I don\\'t plan to be one of those people who, as soon as they turn 18, suddenly buy themselves a massive sports car collection or something similar,\" he told an Australian interviewer earlier this month. \"I don\\'t think I\\'ll be particularly extravagant. \"The things I like buying are things that cost about 10 pounds -- books and CDs and DVDs.\" At 18, Radcliffe will be able to gamble in a casino, buy a drink in a pub or see the horror film \"Hostel: Part II,\" currently six places below his number one movie on the UK box office chart. Details of how he\\'ll mark his landmark birthday are under wraps. His agent and publicist had no comment on his plans. \"I\\'ll definitely have some sort of party,\" he said in an interview. \"Hopefully none of you will be reading about it.\" Radcliffe\\'s earnings from the first five Potter films have been held in a trust fund which he has not been able to touch. Despite his growing fame and riches, the actor says he is keeping his feet firmly on the ground. \"People are always looking to say \\'kid star goes off the rails,\\'\" he told reporters last month. \"But I try very hard not to go that way because it would be too easy for them.\" His latest outing as the boy wizard in \"Harry Potter and the Order of the Phoenix\" is breaking records on both sides of the Atlantic and he will reprise the role in the last two films.  Watch I-Reporter give her review of Potter\\'s latest Â» . There is life beyond Potter, however. The Londoner has filmed a TV movie called \"My Boy Jack,\" about author Rudyard Kipling and his son, due for release later this year. He will also appear in \"December Boys,\" an Australian film about four boys who escape an orphanage. Earlier this year, he made his stage debut playing a tortured teenager in Peter Shaffer\\'s \"Equus.\" Meanwhile, he is braced for even closer media scrutiny now that he\\'s legally an adult: \"I just think I\\'m going to be more sort of fair game,\" he told Reuters. E-mail to a friend . Copyright 2007 Reuters. All rights reserved.This material may not be published, broadcast, rewritten, or redistributed.', 'highlights': \"Harry Potter star Daniel Radcliffe gets Â£20M fortune as he turns 18 Monday .\\nYoung actor says he has no plans to fritter his cash away .\\nRadcliffe's earnings from first five Potter films have been held in trust fund .\", 'id': '42c027e4ff9730fbb3de84c1af0d2c506e41c3e4'}\n"
     ]
    }
   ],
   "source": [
    "#loading the dataset\n",
    "from datasets import load_dataset\n",
    "dataset = load_dataset(\"cnn_dailymail\", \"3.0.0\")\n",
    "print(dataset[\"train\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bc98e02f-c642-489f-977d-37d3206029a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#preprocessing and tokenization\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "model_name = \"tiiuae/falcon-rw-1b\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "db2076cf-61fb-41ac-9184-bed2b5c20080",
   "metadata": {},
   "outputs": [],
   "source": [
    "#formatting the dataset\n",
    "def format_sample(example):\n",
    "    input_text = f\"Summarize the following article:\\n\\n{example['article']}\\n\\nSummary:\"\n",
    "    target_text = example[\"highlights\"]\n",
    "    return {\"prompt\": input_text, \"target\": target_text}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "daeb864f-4e9b-432b-807d-59b7b982e6a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac287795b5dc46f58049d4cc31f0f357",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/287113 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef94756c6df042e29b02a97abdb704b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/13368 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "93e2b685c91d49279dd867010b172b91",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/11490 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "formatted_dataset = dataset.map(format_sample)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9dfa7f8c-f107-4e75-ad95-4ebf42551451",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenizing the dataset\n",
    "def tokenize_function(example):\n",
    "    full_text = example[\"prompt\"] + \" \" + example[\"target\"]\n",
    "    \n",
    "    tokenized = tokenizer(\n",
    "        full_text,\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        max_length=512,\n",
    "    )\n",
    "    prompt_len = len(tokenizer(example[\"prompt\"], truncation=True, max_length=512)[\"input_ids\"])\n",
    "    labels = [-100] * prompt_len + tokenized[\"input_ids\"][prompt_len:]\n",
    "    labels = labels[:512]\n",
    "    labels += [-100] * (512 - len(labels))  # pad if needed\n",
    "    \n",
    "    tokenized[\"labels\"] = labels\n",
    "    return tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e1c5293-2bfe-48ec-a089-74bfcf7bf56a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "269c1dac6e5d438ab131cee93f714ffe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/287113 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_dataset = {\n",
    "    split: formatted_dataset[split].map(\n",
    "        tokenize_function,\n",
    "        batched=False,\n",
    "        remove_columns=formatted_dataset[split].column_names,\n",
    "    )\n",
    "    for split in formatted_dataset\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d60fb01d-2f16-4edd-8c1b-bdb799ac9a3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The installed version of bitsandbytes was compiled without GPU support. 8-bit optimizers and GPU quantization are unavailable.\n",
      "None of the available devices `available_devices = None` are supported by the bitsandbytes version you have installed: `bnb_supported_devices = {'\"cpu\" (needs an Intel CPU and intel_extension_for_pytorch installed and compatible with the PyTorch version)', 'mps', 'npu', 'hpu', 'xpu', 'cuda'}`. Please check the docs to see if the backend you intend to use is available and how to install it: https://huggingface.co/docs/bitsandbytes/main/en/installation#multi-backend\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "None of the available devices `available_devices = None` are supported by the bitsandbytes version you have installed: `bnb_supported_devices = {'\"cpu\" (needs an Intel CPU and intel_extension_for_pytorch installed and compatible with the PyTorch version)', 'mps', 'npu', 'hpu', 'xpu', 'cuda'}`. Please check the docs to see if the backend you intend to use is available and how to install it: https://huggingface.co/docs/bitsandbytes/main/en/installation#multi-backend",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 17\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m     10\u001b[0m bnb_config \u001b[38;5;241m=\u001b[39m BitsAndBytesConfig(\n\u001b[0;32m     11\u001b[0m     load_in_4bit\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m     12\u001b[0m     bnb_4bit_use_double_quant\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m     13\u001b[0m     bnb_4bit_quant_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnf4\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     14\u001b[0m     bnb_4bit_compute_dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat16\n\u001b[0;32m     15\u001b[0m )\n\u001b[1;32m---> 17\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mAutoModelForCausalLM\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     18\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtiiuae/falcon-rw-1b\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     19\u001b[0m \u001b[43m    \u001b[49m\u001b[43mquantization_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbnb_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     20\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mauto\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     21\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[0;32m     22\u001b[0m \u001b[43m)\u001b[49m\n\u001b[0;32m     24\u001b[0m peft_config \u001b[38;5;241m=\u001b[39m LoraConfig(\n\u001b[0;32m     25\u001b[0m     r\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m8\u001b[39m,\n\u001b[0;32m     26\u001b[0m     lora_alpha\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m16\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     29\u001b[0m     task_type\u001b[38;5;241m=\u001b[39mTaskType\u001b[38;5;241m.\u001b[39mCAUSAL_LM\n\u001b[0;32m     30\u001b[0m )\n\u001b[0;32m     32\u001b[0m model \u001b[38;5;241m=\u001b[39m get_peft_model(model, peft_config)\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\llm-env\\lib\\site-packages\\transformers\\models\\auto\\auto_factory.py:564\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[0;32m    562\u001b[0m     \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39mregister(config\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m, model_class, exist_ok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m    563\u001b[0m     model_class \u001b[38;5;241m=\u001b[39m add_generation_mixin_to_remote_model(model_class)\n\u001b[1;32m--> 564\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m model_class\u001b[38;5;241m.\u001b[39mfrom_pretrained(\n\u001b[0;32m    565\u001b[0m         pretrained_model_name_or_path, \u001b[38;5;241m*\u001b[39mmodel_args, config\u001b[38;5;241m=\u001b[39mconfig, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mhub_kwargs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    566\u001b[0m     )\n\u001b[0;32m    567\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(config) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[0;32m    568\u001b[0m     model_class \u001b[38;5;241m=\u001b[39m _get_model_class(config, \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping)\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\llm-env\\lib\\site-packages\\transformers\\modeling_utils.py:309\u001b[0m, in \u001b[0;36mrestore_default_torch_dtype.<locals>._wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    307\u001b[0m old_dtype \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mget_default_dtype()\n\u001b[0;32m    308\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 309\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    310\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    311\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_default_dtype(old_dtype)\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\llm-env\\lib\\site-packages\\transformers\\modeling_utils.py:4390\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, weights_only, *model_args, **kwargs)\u001b[0m\n\u001b[0;32m   4387\u001b[0m     hf_quantizer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   4389\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m hf_quantizer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 4390\u001b[0m     \u001b[43mhf_quantizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalidate_environment\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   4391\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtorch_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4392\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfrom_tf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfrom_tf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4393\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfrom_flax\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfrom_flax\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4394\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4395\u001b[0m \u001b[43m        \u001b[49m\u001b[43mweights_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweights_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4396\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   4397\u001b[0m     torch_dtype \u001b[38;5;241m=\u001b[39m hf_quantizer\u001b[38;5;241m.\u001b[39mupdate_torch_dtype(torch_dtype)\n\u001b[0;32m   4398\u001b[0m     device_map \u001b[38;5;241m=\u001b[39m hf_quantizer\u001b[38;5;241m.\u001b[39mupdate_device_map(device_map)\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\llm-env\\lib\\site-packages\\transformers\\quantizers\\quantizer_bnb_4bit.py:84\u001b[0m, in \u001b[0;36mBnb4BitHfQuantizer.validate_environment\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     81\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m is_bitsandbytes_multi_backend_available\n\u001b[0;32m     83\u001b[0m bnb_multibackend_is_enabled \u001b[38;5;241m=\u001b[39m is_bitsandbytes_multi_backend_available()\n\u001b[1;32m---> 84\u001b[0m \u001b[43mvalidate_bnb_backend_availability\u001b[49m\u001b[43m(\u001b[49m\u001b[43mraise_exception\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m     86\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfrom_tf\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m) \u001b[38;5;129;01mor\u001b[39;00m kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfrom_flax\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m     87\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m     88\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConverting into 4-bit or 8-bit weights from tf/flax weights is currently not supported, please make\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     89\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m sure the weights are in PyTorch format.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     90\u001b[0m     )\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\llm-env\\lib\\site-packages\\transformers\\integrations\\bitsandbytes.py:560\u001b[0m, in \u001b[0;36mvalidate_bnb_backend_availability\u001b[1;34m(raise_exception)\u001b[0m\n\u001b[0;32m    557\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_bitsandbytes_multi_backend_available():\n\u001b[1;32m--> 560\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_validate_bnb_multi_backend_availability\u001b[49m\u001b[43m(\u001b[49m\u001b[43mraise_exception\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    561\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _validate_bnb_cuda_backend_availability(raise_exception)\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\llm-env\\lib\\site-packages\\transformers\\integrations\\bitsandbytes.py:517\u001b[0m, in \u001b[0;36m_validate_bnb_multi_backend_availability\u001b[1;34m(raise_exception)\u001b[0m\n\u001b[0;32m    511\u001b[0m     err_msg \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    512\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNone of the available devices `available_devices = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mavailable_devices\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01mor\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m` are supported by the bitsandbytes version you have installed: `bnb_supported_devices = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbnb_supported_devices_with_info\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m`. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    513\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease check the docs to see if the backend you intend to use is available and how to install it: https://huggingface.co/docs/bitsandbytes/main/en/installation#multi-backend\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    514\u001b[0m     )\n\u001b[0;32m    516\u001b[0m     logger\u001b[38;5;241m.\u001b[39merror(err_msg)\n\u001b[1;32m--> 517\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(err_msg)\n\u001b[0;32m    519\u001b[0m logger\u001b[38;5;241m.\u001b[39mwarning(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo supported devices found for bitsandbytes multi-backend.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    520\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: None of the available devices `available_devices = None` are supported by the bitsandbytes version you have installed: `bnb_supported_devices = {'\"cpu\" (needs an Intel CPU and intel_extension_for_pytorch installed and compatible with the PyTorch version)', 'mps', 'npu', 'hpu', 'xpu', 'cuda'}`. Please check the docs to see if the backend you intend to use is available and how to install it: https://huggingface.co/docs/bitsandbytes/main/en/installation#multi-backend"
     ]
    }
   ],
   "source": [
    "#loading the model and setting the Qlora configs\n",
    "from transformers import AutoModelForCausalLM, BitsAndBytesConfig\n",
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "import torch\n",
    "\n",
    "from transformers import AutoModelForCausalLM, BitsAndBytesConfig\n",
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "import torch\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"tiiuae/falcon-rw-1b\",\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "peft_config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.1,\n",
    "    bias=\"none\",\n",
    "    task_type=TaskType.CAUSAL_LM\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, peft_config)\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7e9f2910-54ab-4484-8ee2-ccf25c81a887",
   "metadata": {},
   "outputs": [],
   "source": [
    "#data collator\n",
    "from transformers import DataCollatorForLanguageModeling\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm=False  # This is a causal LM, so we don't use Masked LM\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1235dedb-8874-4593-a0ef-6ea8976324f2",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m model \u001b[38;5;241m=\u001b[39m get_peft_model(\u001b[43mmodel\u001b[49m, peft_config)\n\u001b[0;32m      2\u001b[0m model\u001b[38;5;241m.\u001b[39mprint_trainable_parameters()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "#training argument\n",
    "from transformers import TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./falcon-qlora-summarizer\",\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    gradient_accumulation_steps=4,\n",
    "    num_train_epochs=1,\n",
    "    logging_steps=10,\n",
    "    evaluation_strategy=\"no\",   # or \"epoch\" if you have validation\n",
    "    save_strategy=\"epoch\",\n",
    "    save_total_limit=2,\n",
    "    fp16=True,  # Use mixed precision\n",
    "    report_to=\"none\",\n",
    "    remove_unused_columns=False,  # Important for PEFT models\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "714390ab-f859-420a-ae81-a018b37ae386",
   "metadata": {},
   "outputs": [],
   "source": [
    "#trainer engine\n",
    "from transformers import Trainer\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset[\"train\"].shuffle(seed=42).select(range(1000)),  # use subset for fast testing\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f84dd569-cfdf-47f4-b729-be38fa299038",
   "metadata": {},
   "outputs": [],
   "source": [
    "#training/finetuning the model\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11e466de-df03-4a56-920e-4619cc0bde1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#saving the model\n",
    "model.save_pretrained(\"falcon-qlora-summarizer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fef27a7-28e1-438e-84d3-7e328fd0a77a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the tokenizer\n",
    "tokenizer.save_pretrained(\"falcon-qlora-summarizer\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
